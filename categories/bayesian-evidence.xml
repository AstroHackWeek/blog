<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" version="2.0"><channel><title>Astro Hack Week (bayesian evidence)</title><link>http://astrohackweek.github.io/</link><description></description><atom:link type="application/rss+xml" rel="self" href="http://astrohackweek.github.io/blog/categories/bayesian-evidence.xml"></atom:link><language>en</language><lastBuildDate>Tue, 06 Oct 2015 04:03:45 GMT</lastBuildDate><generator>https://getnikola.com/</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Bayesian Evidence Calculation</title><link>http://astrohackweek.github.io/blog/bayesian-evidence.html</link><dc:creator>Kyle Barbary</dc:creator><description>&lt;div&gt;&lt;p&gt;In a Bayesian framework, object classification or model comparison can
be done naturally by comparing the Bayesian &lt;em&gt;evidence&lt;/em&gt; between two or
more models, given the data. The evidence is the integral of the
likelihood of the data over the entire prior volume for all the model
parameters, weighted by the prior. (The ratio of evidence for two
different models is known as the &lt;a href="http://en.wikipedia.org/wiki/Bayes_factor"&gt;Bayes
Factor&lt;/a&gt;.) This
multi-dimensional integral gets increasingly computationally intensive
as the number of parameters increases. As a result, several clever
algorithms have been developed to efficiently approximate the answer.&lt;/p&gt;
&lt;p&gt;In this hack, I looked at a couple specific implementations of such
algorithms in Python.&lt;/p&gt;
&lt;p&gt;&lt;a href="http://astrohackweek.github.io/blog/bayesian-evidence.html"&gt;Read moreâ€¦&lt;/a&gt; (3 min remaining to read)&lt;/p&gt;&lt;/div&gt;</description><category>bayesian evidence</category><category>hacking</category><guid>http://astrohackweek.github.io/blog/bayesian-evidence.html</guid><pubDate>Fri, 03 Oct 2014 15:00:00 GMT</pubDate></item></channel></rss>